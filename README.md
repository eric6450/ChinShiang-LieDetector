# ChinShiang-LieDetector
---
type: "project" # DON'T TOUCH THIS ! :)
date: "2023-06-13" # Date you first upload your project.
# Title of your project (we like creative title)
title: "Using Multimodal data and Machine Learning to Perform Lie Detection"

# List the names of the collaborators within the [ ]. If alone, simple put your name within []
names: [ChinShiang Ma]

# Your project GitHub repository URL
github_repo: https://github.com/eric6450/ChinShiang-LieDetector

# If you are working on a project that has website, indicate the full url including "https://" below or leave it empty.
website:

# List +- 4 keywords that best describe your project within []. Note that the project summary also involves a number of key words. Those are listed on top of the [github repository](https://github.com/eric6450/ChinShiang-LieDetector.git), click `manage topics`.
# Please only lowercase letters
tags: [project, github, markdown, brainhack]

# Summarize your project in < ~75 words. This description will appear at the top of your page and on the list page with other projects..

summary: "We use lie/truth EEG dataset to train the MLP、LSTM、CNN model and use the model to test the unseen dataset, LSTM has the best performance of the tree."

# If you want to add a cover image (listpage and image in the right), add it to your directory and indicate the name
# below with the extension.
image: ""
---
<!-- This is an html comment and this won't appear in the rendered page. You are now editing the "content" area, the core of your description. Everything that you can do in markdown is allowed below. We added a couple of comments to guide your through documenting your progress. -->

## Project definition

### Background

Inspired by the drama "Lie to me" , we wander if we can use EEG as well as the micro expression and eye tracking data to detect the lie. Due to the EEG dataset is the only one we can acquire now , so we test the DL model first. In the future if we can get all the dataset we need , we would try a multimodal model to detect the lie"

<iframe width="560" height="315" src="https://www.youtube.com/embed/PTYs_JFKsHI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Tools

The "Lie detector" project will rely on the following technologies:
 * MLP/DNN model.
 * LSTM
 * CNN

### Data

EEG dataset of lie and truth from Gao, Junfeng et al. (2015), Data from: A novel algorithm to enhance P300
in single trials:application to lie detection using F-score and SVM, Dataset.

### Deliverables

At the end of this project, we will have:
 - The program on the google colab : https://colab.research.google.com/drive/1MtS7uJk5bjY1nsclsQb0TZSzsC054fRC?usp=sharing
 - A gallery of the student projects at Brainhack 2023.
 - Instructions on the website about how to submit a pull request to the [brainhack school website](https://github.com/eric6450/ChinShiang-LieDetector) in order to add the project description to the website.

## Results

### Progress overview

The project was very primi

### Tools I learned during this project

 * **Meta-project** P Bellec learned how to do a meta project for the first time, which is developping a framework while using it at the same time. It felt really weird, but somehow quite fun as well.
 * **Github workflow-** The successful use of this template approach will demonstrate that it is possible to incorporate dozens of students presentation on a website collaboratively over a few weeks.
 * **Project content** Through the project reports generated using the template, it is possible to learn about what exactly the brainhack school students are working on.

### Results

#### Deliverable 1: report template

You are currently reading the report template! I will let you judge whether it is useful or not. If you think there is something that could be improved, please do not hesitate to open an issue [here](https://github.com/eric6450/ChinShiang-LieDetector/issue) and let us know.

#### Deliverable 2: project colab

You can check out the https://colab.research.google.com/drive/1MtS7uJk5bjY1nsclsQb0TZSzsC054fRC?usp=sharing

## Conclusion and acknowledgement

Data is always the key , how to get and combine the multimodal data is the key point
Due to the limited time and resources , currently , I just finished the EEG part, if I have more time and resources in the future, I would like to build this foundation models and see how good it works
learns a lot and enjoy the progress in this course , thanks all teachers and TAs


